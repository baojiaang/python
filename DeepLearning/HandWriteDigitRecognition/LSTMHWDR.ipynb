{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# training  data增强\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(), # 水平翻转\n",
    "    transforms.RandomRotation(15), # 翻转\n",
    "    transforms.ToTensor(), # 图片 to Tensor，normalize\n",
    "])\n",
    "# test transform\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "# 调用预置的dataset\n",
    "\n",
    "train_set = datasets.MNIST(root=r'G:\\研一\\深度学习\\handwritedigit',\n",
    "                           transform = train_transform,\n",
    "                            train = True,\n",
    "                            download=False\n",
    "                            )\n",
    "\n",
    "test_set = datasets.MNIST(root=r'G:\\研一\\深度学习\\handwritedigit',\n",
    "                           transform = test_transform,\n",
    "                           train=False\n",
    "                           )\n",
    "def evaluation(outputs, labels):\n",
    "    pred_y = torch.max(outputs,1)[1].data.cpu().numpy().squeeze()\n",
    "    correct = sum(pred_y == labels.cpu().numpy()).item()\n",
    "    return correct"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data.train_data.size(): torch.Size([60000, 28, 28])\n",
      "train_data.train_labels.size(): torch.Size([60000])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "BATCH_SIZE = 64\n",
    "INPUT_SIZE = 28\n",
    "TIME_STEP = 28\n",
    "print('train_data.train_data.size():', train_set.data.size())    # 打印训练集特征的size\n",
    "print('train_data.train_labels.size():', train_set.targets.size())    # 打印训练集标签的size\n",
    "\n",
    "# 先归一化，在分割数据，为节约时间，只取了部分\n",
    "data_x = train_set.data.type(torch.FloatTensor)/255.\n",
    "data_y = train_set.targets.type(torch.Tensor)\n",
    "#\n",
    "train_x = data_x[:50000]\n",
    "train_y = data_y[:50000]\n",
    "valid_x = data_x[50000:]\n",
    "valid_y = data_y[50000:]\n",
    "# test_x = data_x[12000:14000]\n",
    "# test_y = data_y[12000:14000]\n",
    "#\n",
    "data_train = list(train_x.numpy().reshape(1,-1, TIME_STEP, INPUT_SIZE))  # 使用list只会把最外层变为list，内层还是ndarray，和.tolist()方法不同\n",
    "data_valid = list(valid_x.numpy().reshape(1,-1, TIME_STEP, INPUT_SIZE))\n",
    "# data_test = list(test_x.numpy().reshape(1,-1, TIME_STEP, INPUT_SIZE))\n",
    "data_train.append(list(train_y.numpy().reshape(-1,1)))\n",
    "data_valid.append(list(valid_y.numpy().reshape(-1,1)))\n",
    "# data_test.append(list(test_y.numpy().reshape(-1,1)))\n",
    "#\n",
    "data_train = list(zip(*data_train))   # 最外层是list，次外层是tuple，内层都是ndarray\n",
    "data_valid = list(zip(*data_valid))\n",
    "# data_test = list(zip(*data_test))\n",
    "#\n",
    "train_loader = DataLoader(data_train, batch_size=BATCH_SIZE, num_workers=8, pin_memory=True, shuffle=True)\n",
    "valid_loader = DataLoader(data_valid, batch_size=BATCH_SIZE, num_workers=8, pin_memory=True, shuffle=True)\n",
    "# test_loader = DataLoader(data_test, batch_size=BATCH_SIZE, num_workers=8, pin_memory=True, shuffle=False)\n",
    "# load data\n",
    "# train_loader = torch.utils.data.DataLoader(dataset =train_set,\n",
    "#                                                 batch_size = 64,\n",
    "#                                                 shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset =test_set,\n",
    "                                                batch_size = 64,\n",
    "                                                shuffle = False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,embedding_dim, hidden_dim, num_layers, dropout=0.5):\n",
    "        super(Model,self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=hidden_dim,num_layers=num_layers,batch_first=True)\n",
    "\n",
    "        self.classifier = nn.Sequential( #nn.Dropout(dropout),\n",
    "                                         nn.Linear(hidden_dim, 10),\n",
    "                                         #nn.ReLU()\n",
    "                                        )\n",
    "    def forward(self, x):\n",
    "        # output, (h_n, c_n)\n",
    "        # output features from last layer of the LSTM\n",
    "        # h_n tensor containing the hidden state for t = seq_len\n",
    "        # c_n tensor containing the cell state for t = seq_len\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # x 的 dimension (batch, seq_len, hidden_size)\n",
    "        # 取用 LSTM 最後一層的 hidden state   ??\n",
    "        #  x = h_n[-1, :, :]   或者这样\n",
    "        lstm_out = lstm_out[:, -1, :]      # LSTM 最后一层的 hidden state\n",
    "        lstm_out = self.classifier(lstm_out)\n",
    "        return lstm_out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def training(batch_size, n_epoch, lr, model_dir, train, valid, model, device):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print('\\nstart training, parameter total:{}, trainable:{}\\n'.format(total, trainable))\n",
    "    model.train() # 將 model 的模式設為 train，這樣 optimizer 就可以更新 model 的參數\n",
    "    criterion = nn.CrossEntropyLoss() #\n",
    "    t_batch = len(train)\n",
    "    v_batch = len(valid)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr) # 將模型的參數給 optimizer，並給予適當的 learning rate\n",
    "    total_loss, total_acc, best_acc = 0, 0, 0\n",
    "    for epoch in range(n_epoch):\n",
    "        total_loss, total_acc = 0, 0\n",
    "        # 這段做 training\n",
    "        for i, (x, y) in enumerate(train):      #  for i, data in enumerate(train)   data[0]  inputs data[1] labels\n",
    "            x = x.to(device, dtype=torch.float) # device 為 \"cuda\"，將 inputs 轉成 torch.cuda.LongTensor\n",
    "            y = y.to(device, dtype=torch.long)# device為 \"cuda\"，將 labels 轉成 torch.cuda.FloatTensor，因為等等要餵進 criterion，所以型態要是 float\n",
    "            x = x.squeeze()\n",
    "            y = y.squeeze()\n",
    "            y_pred = model(x) # 將 input 餵給模型\n",
    "            #y_pred = y_pred.squeeze() # 去掉最外面的 dimension，好讓 outputs 可以餵進 criterion()\n",
    "            # print(y_pred.shape)\n",
    "            # print(y.shape)\n",
    "            loss = criterion(y_pred, y) # 計算此時模型的 training loss\n",
    "            optimizer.zero_grad() # 由於 loss.backward() 的 gradient 會累加，所以每次餵完一個 batch 後需要歸零\n",
    "            loss.backward() # 算 loss 的 gradient\n",
    "            optimizer.step() # 更新訓練模型的參數\n",
    "            correct = evaluation(y_pred, y) # 計算此時模型的 training accuracy\n",
    "            total_acc += (correct / batch_size)\n",
    "            total_loss += loss.item()\n",
    "        print('[ Epoch{}: {}/{} ] loss:{:.3f} acc:{:.3f} '.format(\n",
    "                epoch+1, i+1, t_batch, loss.item(), correct*100/batch_size), end='\\r')\n",
    "    print('\\nTrain | Loss:{:.5f} Acc: {:.3f}'.format(total_loss/t_batch, total_acc/t_batch*100))\n",
    "        # 這段做 validation\n",
    "    model.eval() # 將 model 的模式設為 eval，這樣 model 的參數就會固定住\n",
    "    with torch.no_grad():\n",
    "        total_loss, total_acc = 0, 0\n",
    "        for i, (in_x, labels) in enumerate(valid):\n",
    "            in_x = in_x.to(device, dtype=torch.float) # device 為 \"cuda\"，將 inputs 轉成 torch.cuda.LongTensor\n",
    "            labels = labels.to(device, dtype=torch.long)# device 為 \"cuda\"，將 labels 轉成 torch.cuda.FloatTensor，因為等等要餵進 criterion，所以型態要是 float\n",
    "            in_x = in_x.squeeze()\n",
    "            labels = labels.squeeze()\n",
    "            y_pred = model(in_x) # 將 input 餵給模型\n",
    "            #y_pred = y_pred.squeeze() # 去掉最外面的 dimension，好讓 outputs 可以餵進 criterion()\n",
    "            loss = criterion(y_pred, labels) # 計算此時模型的 validation loss\n",
    "            correct = evaluation(y_pred, labels) # 計算此時模型的 validation accuracy\n",
    "            total_acc += (correct / batch_size)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(\"Valid | Loss:{:.5f} Acc: {:.3f} \".format(total_loss/v_batch, total_acc/v_batch*100))\n",
    "        if total_acc > best_acc:\n",
    "            # 如果 validation 的結果優於之前所有的結果，就把當下的模型存下來以備之後做預測時使用\n",
    "            best_acc = total_acc\n",
    "            #torch.save(model, \"{}/val_acc_{:.3f}.model\".format(model_dir,total_acc/v_batch*100))\n",
    "            torch.save(model, \"{}/lstm.model\".format(model_dir))\n",
    "            print('saving model with acc {:.3f}'.format(total_acc/v_batch*100))\n",
    "        print('-----------------------------------------------')\n",
    "        model.train() # 將 model 的模式設為 train，這樣 optimizer 就可以更新 model 的參數（因為剛剛轉成 eval 模式）"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def testing(batch_size, test_loader, model, device):\n",
    "    model.eval()\n",
    "    ret_output = []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    v_test = len(test_loader)\n",
    "    with torch.no_grad():\n",
    "         total_loss, total_acc = 0, 0\n",
    "         for i, (in_x, labels) in enumerate(test_loader):\n",
    "            in_x = in_x.to(device, dtype=torch.long) # device 為 \"cuda\"，將 inputs 轉成 torch.cuda.LongTensor\n",
    "            labels = labels.to(device, dtype=torch.float) # device 為 \"cuda\"，將 labels 轉成 torch.cuda.FloatTensor，因為等等要餵進 criterion，所以型態要是 float\n",
    "            y_pred = model(in_x) # 將 input 餵給模型\n",
    "            y_pred = y_pred.squeeze() # 去掉最外面的 dimension，好讓 outputs 可以餵進 criterion()\n",
    "            loss = criterion(y_pred, labels) # 計算此時模型的 validation loss\n",
    "            correct = evaluation(y_pred, labels) # 計算此時模型的 validation accuracy\n",
    "            total_acc += (correct / batch_size)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            print(\"Valid | Loss:{:.5f} Acc: {:.3f} \".format(total_loss/v_test, total_acc/v_test*100))\n",
    "    return ret_output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "start training, parameter total:24714, trainable:24714\n",
      "\n",
      "[ Epoch5: 782/782 ] loss:0.282 acc:21.875 \r\n",
      "Train | Loss:0.09336 Acc: 97.093\n",
      "Valid | Loss:0.09460 Acc: 96.766 \n",
      "saving model with acc 96.766\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Model. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# main\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 64\n",
    "epoch = 5\n",
    "lr = 0.001\n",
    "model = Model(embedding_dim=28, hidden_dim=64, num_layers=1, dropout=0.5)\n",
    "model = model.to(device)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    training(batch_size, epoch, lr, r'G:\\研一\\深度学习\\handwritedigit', train_loader, valid_loader, model, device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}